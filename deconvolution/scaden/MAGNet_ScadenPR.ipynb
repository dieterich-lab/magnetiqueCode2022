{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaden is a deep-learning based algorithm for cell type deconvolution of bulk RNA-seq samples.\n",
    "\n",
    "## PR\n",
    "\n",
    "### Bulk RNA-seq\n",
    "\n",
    "MAGNet\n",
    "\n",
    "### scRNA-seq and snRNA-seq\n",
    "\n",
    "Processed scRNA-seq data GEO accession codes GSE109816 and GSE121893 (data re-analysed).\n",
    "\n",
    "Processed scRNA- and snRNA-seq data from https://www.heartcellatlas.org\n",
    "\n",
    "Processed snRNA-seq data from https://singlecell.broadinstitute.org/single_cell/study/SCP498/transcriptional-and-cellular-diversity-of-the-human-heart#study-download\n",
    "\n",
    "\n",
    "scRNA- and snRNA-seq pre-processed (deconvolution_prep_scdata.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import order is important to avoid ImportError\n",
    "import sklearn\n",
    "\n",
    "import scaden\n",
    "\n",
    "# run scaden as module\n",
    "from scaden import example as scx\n",
    "from scaden import process as scp\n",
    "from scaden import train as sct\n",
    "from scaden import predict as scpr\n",
    "from scaden import simulate as scs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/prj/MAGE/analysis/deconvolution/scaden/.pml_pr/lib/python3.6/site-packages/numba/np/ufunc/parallel.py:365: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 7003. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import loompy as lp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/eboileau/.virtualenvs/pml/lib/python3.6/site-packages/numba/np/ufunc/parallel.py:365: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 7003. The TBB threading layer is disabled.\n",
    "  warnings.warn(problem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using normalized data (no log transformation) - raw counts in \"counts\" layer\n",
    "scDir = '/prj/MAGE/analysis/deconvolution/scdata/scaden'\n",
    "# raw - we need to Scanpy normalize the same way\n",
    "bulkDir = '/prj/MAGE/analysis/data/stringtie'\n",
    "\n",
    "locDir = '/prj/MAGE/analysis/deconvolution/scaden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to leverage the heterogeneity of multisubject data, training data is generated separately for\n",
    "# every sample in the dataset\n",
    "\n",
    "# see wrap_scaden_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk data - genes x samples\n",
    "lf = lp.connect(glob.glob(os.path.join(bulkDir, '*.loom'))[0], mode='r+', validate=False)\n",
    "# Only one indexing vector or array is currently allowed for advanced selection\n",
    "X = lf[lf.ra['GeneFlag']==1,:]\n",
    "X = X[:,lf.ca['SampleFlag']==1]\n",
    "# same as scanpy.pp.normalize_total with exclude_highly_expressed=False\n",
    "def _normalize_data(X, counts, after=None, copy=True):\n",
    "    X = X.copy() if copy else X\n",
    "    if issubclass(X.dtype.type, (int, np.integer)):\n",
    "        X = X.astype(np.float32)  # TODO: Check if float64 should be used\n",
    "    counts = np.asarray(counts)  # dask doesn't do medians\n",
    "    after = np.median(counts[counts>0], axis=0) if after is None else after\n",
    "    counts += (counts == 0)\n",
    "    counts = counts / after\n",
    "    # no sparse data\n",
    "    np.divide(X, counts[:, None], out=X)\n",
    "    return X\n",
    "\n",
    "target_sum = 1e4\n",
    "counts_per_cell = X.sum(1)\n",
    "cell_subset = counts_per_cell > 0\n",
    "if not np.all(cell_subset):\n",
    "    print('Some samples have total count of genes equal to zero!')\n",
    "else:\n",
    "    X = _normalize_data(X, counts_per_cell, target_sum)\n",
    "\n",
    "    \n",
    "bulkf = os.path.join(locDir, 'results', 'MAGNet_counts.txt')\n",
    "pd.DataFrame(X, \n",
    "             index=lf.ra[lf.ra['GeneFlag']==1]['Gene'], \n",
    "             columns=lf.ca[lf.ca['SampleFlag']==1]['CellID']).to_csv(bulkf, \n",
    "                                                                     index=True, \n",
    "                                                                     header=True, \n",
    "                                                                     sep='\\t',\n",
    "                                                                     float_format='%.5f')\n",
    "lf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scs.simulation(simulate_dir=os.path.join(locDir, 'training_/'), # trailing / is important!\n",
    "#                data_dir=os.path.join(locDir, 'input/'),\n",
    "#                sample_size=1000, # number of cells per sample\n",
    "#                num_samples=2000, # number of samples\n",
    "#                pattern=\"HCASampleH3.h5ad\",\n",
    "#                unknown_celltypes=['unknown'], # must be a list, we don't have any\n",
    "#                out_prefix='test',\n",
    "#                fmt='h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SCP498Sample1723 dataset ...\n",
      "Loading SCP498Sample1681 dataset ...\n",
      "Loading GSE109816SampleN2 dataset ...\n",
      "Loading GSE109816SampleD4 dataset ...\n",
      "Loading GSE109816SampleN9 dataset ...\n",
      "Loading HCASampleD1 dataset ...\n",
      "Loading HCASampleD2 dataset ...\n",
      "Loading HCASampleH4 dataset ...\n",
      "Loading GSE109816SampleN5 dataset ...\n",
      "Loading SCP498Sample1702 dataset ...\n",
      "Loading GSE109816SampleN10 dataset ...\n",
      "Loading HCASampleH6 dataset ...\n",
      "Loading GSE109816SampleN12 dataset ...\n",
      "Loading GSE109816SampleN8 dataset ...\n",
      "Loading GSE109816SampleN11 dataset ...\n",
      "Loading HCASampleD6 dataset ...\n",
      "Loading SCP498Sample1666 dataset ...\n",
      "Loading HCASampleH3 dataset ...\n",
      "Loading GSE109816SampleN13 dataset ...\n",
      "Loading GSE109816SampleN14 dataset ...\n",
      "Loading HCASampleD4 dataset ...\n",
      "Loading GSE109816SampleN1 dataset ...\n",
      "Loading HCASampleH7 dataset ...\n",
      "Loading HCASampleH2 dataset ...\n",
      "Loading GSE109816SampleC2 dataset ...\n",
      "Loading GSE109816SampleC1 dataset ...\n",
      "Loading HCASampleH5 dataset ...\n",
      "Loading HCASampleD5 dataset ...\n",
      "Loading GSE109816SampleN7 dataset ...\n",
      "Loading SCP498Sample1708 dataset ...\n",
      "Loading GSE109816SampleN3 dataset ...\n",
      "Loading GSE109816SampleN4 dataset ...\n",
      "Loading SCP498Sample1221 dataset ...\n",
      "Loading GSE109816SampleD2 dataset ...\n",
      "Loading GSE109816SampleD1 dataset ...\n",
      "Loading GSE109816SampleD5 dataset ...\n",
      "Loading HCASampleD3 dataset ...\n",
      "Loading HCASampleD7 dataset ...\n",
      "Loading HCASampleD11 dataset ...\n",
      "Loading GSE109816SampleN6 dataset ...\n",
      "Loading SCP498Sample1600 dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normal samples: 100%|██████████| 1000/1000 [49:28<00:00,  2.97s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [26:21<00:00,  1.58s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [36:36<00:00,  2.20s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [19:45<00:00,  1.19s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:48<00:00,  4.37it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:14<00:00,  3.92it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:56<00:00,  4.23it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:09<00:00,  4.00it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:53<00:00,  4.29it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:14<00:00,  3.92it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [14:28<00:00,  1.15it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [09:21<00:00,  1.78it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [33:08<00:00,  1.99s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [18:44<00:00,  1.12s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [21:20<00:00,  1.28s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [12:40<00:00,  1.32it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [04:02<00:00,  4.12it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:21<00:00,  3.83it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [30:49<00:00,  1.85s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [17:31<00:00,  1.05s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:48<00:00,  4.38it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:22<00:00,  3.81it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [21:42<00:00,  1.30s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [13:19<00:00,  1.25it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:46<00:00,  4.42it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:18<00:00,  3.87it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:44<00:00,  4.45it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:23<00:00,  3.80it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:49<00:00,  4.36it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:13<00:00,  3.94it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [56:31<00:00,  3.39s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [29:09<00:00,  1.75s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [24:20<00:00,  1.46s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [13:57<00:00,  1.19it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [24:48<00:00,  1.49s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [14:29<00:00,  1.15it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:40<00:00,  4.54it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:03<00:00,  4.11it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:28<00:00,  4.79it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:06<00:00,  4.05it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [23:49<00:00,  1.43s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [13:21<00:00,  1.25it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:41<00:00,  4.51it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:05<00:00,  4.08it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [25:34<00:00,  1.53s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [14:13<00:00,  1.17it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [18:44<00:00,  1.12s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [11:20<00:00,  1.47it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:30<00:00,  4.75it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:04<00:00,  4.10it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:59<00:00,  4.17it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:09<00:00,  4.01it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [29:34<00:00,  1.77s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [17:17<00:00,  1.04s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [23:42<00:00,  1.42s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [13:47<00:00,  1.21it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:22<00:00,  4.93it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:03<00:00,  4.10it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [32:06<00:00,  1.93s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [18:20<00:00,  1.10s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:47<00:00,  4.39it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:21<00:00,  3.82it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:33<00:00,  4.68it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:09<00:00,  4.00it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [18:30<00:00,  1.11s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [11:26<00:00,  1.46it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:40<00:00,  4.54it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [03:56<00:00,  4.22it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:28<00:00,  4.80it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [03:51<00:00,  4.31it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [03:14<00:00,  5.14it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [03:52<00:00,  4.31it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [20:51<00:00,  1.25s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [12:24<00:00,  1.34it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [28:09<00:00,  1.69s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [16:13<00:00,  1.03it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [35:47<00:00,  2.15s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [19:35<00:00,  1.18s/it]\n",
      "Normal samples: 100%|██████████| 1000/1000 [04:06<00:00,  4.05it/s]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [04:24<00:00,  3.78it/s]\n",
      "Normal samples: 100%|██████████| 1000/1000 [20:27<00:00,  1.23s/it]\n",
      "Sparse samples: 100%|██████████| 1000/1000 [12:05<00:00,  1.38it/s]\n",
      "/beegfs/prj/MAGE/analysis/deconvolution/scaden/.pml_pr/lib/python3.6/site-packages/anndata/_core/anndata.py:119: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "... storing 'ds' as categorical\n"
     ]
    }
   ],
   "source": [
    "scs.simulation(simulate_dir=os.path.join(locDir, 'training/'), # trailing / is important!\n",
    "               data_dir=os.path.join(locDir, 'input/'),\n",
    "               sample_size=1000, # number of cells per sample\n",
    "               num_samples=2000, # number of samples\n",
    "               pattern=\"*.h5ad\",\n",
    "               unknown_celltypes=['unknown'], # must be a list, we don't have any\n",
    "               out_prefix='trained_3set_by_sample_1000cells_2000samples',\n",
    "               fmt='h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output\n",
    "\n",
    "- `celltypes.txt` which contains all cell types in the training data\n",
    "- `*_labels.txt` for each input sc dataset, of size num_samples × num_celltypes (cell type proportion for each sample)\n",
    "- `*_samples.txt` bulk data simulated for each sc dataset of size num_samples × num_genes (intersect of all sc datasets)\n",
    "- `*.h5ad`, bulk data combined of size n_obs × n_vars = num_scdataset * num_samples × num_genes (intersect of all sc datasets), var are genes, and obs contains for each sample cell type proportions\n",
    "\n",
    "**Note:** `sample_size`\n",
    "\n",
    "For each num_samples (artificial), generate a random fraction of each cell type given sc datasets. Using sample_size (number of cells per sample), fractions are multiplied to obtain the number of each cell\n",
    "type that will be found in each mock bulk sample. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process input\n",
    "\n",
    "- intersection of genes between training and data default var_cutoff=0.1\n",
    "- log2-transformed and scaled\n",
    "- pre-process: default scaling_option cannot be changed uses sklearn.preprocessing.MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction data (bulk)\n",
    "data_path = os.path.join(locDir, 'results', 'MAGNet_counts.txt')\n",
    "# training data (h5ad file) scRNA-seq\n",
    "training_data = os.path.join(locDir, 'training', 'trained_3set_by_sample_1000cells_2000samples.h5ad')\n",
    "# name of processed file - output\n",
    "processed_path = os.path.join(locDir, 'training', 'trained_3set_by_sample_1000cells_2000samples_processed.h5ad')\n",
    "\n",
    "# default\n",
    "var_cutoff = 0.1\n",
    "\n",
    "scp.processing(data_path, training_data, processed_path, var_cutoff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Options:\n",
    "\n",
    "--train_datasets Comma-separated list of datasets used for training.\n",
    "\n",
    "Here, we simulated sc data separately for training. Use multiple sc dataset to simulate one training dataset?\n",
    "Simulate multiple training dataset?\n",
    "\n",
    "- uses 3 deep NN, trained for 5,000 steps\n",
    "- default: --batch_size 128  --learning_rate 0.0001  --steps 5000  --seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 4999, Loss: 0.0021: 100%|██████████| 5000/5000 [03:09<00:00, 26.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /prj/MAGE/analysis/deconvolution/scaden/model//m256/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 4999, Loss: 0.0014: 100%|██████████| 5000/5000 [03:55<00:00, 21.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /prj/MAGE/analysis/deconvolution/scaden/model//m512/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 4999, Loss: 0.0014: 100%|██████████| 5000/5000 [05:12<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /prj/MAGE/analysis/deconvolution/scaden/model//m1024/assets\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(locDir, 'training', 'trained_3set_by_sample_1000cells_2000samples_processed.h5ad')\n",
    "\n",
    "train_datasets = '' # ds from processed, uses all by default when called\n",
    "\n",
    "model_dir = os.path.join(locDir, 'model/')\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001 \n",
    "num_steps = 5000\n",
    "\n",
    "sct.training(data_path,\n",
    "             train_datasets,\n",
    "             model_dir,\n",
    "             batch_size,\n",
    "             learning_rate,\n",
    "             num_steps,\n",
    "             seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(locDir, 'model/')\n",
    "\n",
    "data_path = os.path.join(locDir, 'results', 'MAGNet_counts.txt')\n",
    "\n",
    "out_name = os.path.join(locDir, 'results', 'scaden_deconvolution_MAGNet_counts_trained_3set_by_sample_1000cells_2000samples.txt')\n",
    "\n",
    "scpr.prediction(model_dir=model_dir,\n",
    "                data_path=data_path,\n",
    "                out_name=out_name,\n",
    "                seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anndata import read_h5ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 82000 × 18257\n",
       "    obs: 'Endothelial', 'Fibroblast', 'Macrophage', 'Ventricular_cardiomyocyte', 'Mesothelial', 'Smooth_muscle', 'Atrial_cardiomyocyte', 'Adipocyte', 'Lymphocyte', 'Pericyte', 'Lymphoid', 'Neuronal', 'Myeloid', 'ds', 'batch'\n",
       "    uns: 'cell_types', 'unknown'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = read_h5ad(os.path.join(locDir, 'training', 'trained_3set_by_sample_1000cells_2000samples_processed.h5ad'))\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pml_pr",
   "language": "python",
   "name": ".pml_pr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
